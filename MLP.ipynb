{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8a25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (236607, 480)\n",
      "Total classes: 263\n",
      "Example classes: ['1.1.1' '1.1.2' '1.1.3' '1.1.5' '1.1.7' '1.1.9' '1.1.98' '1.1.99'\n",
      " '1.10.3' '1.10.5' '1.11.1' '1.11.2' '1.12.1' '1.12.2' '1.12.5' '1.12.7'\n",
      " '1.12.98' '1.12.99' '1.13.11' '1.13.12']\n",
      "Train size: 189285\n",
      "Test size : 47322\n",
      "Using device: cuda\n",
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=480, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=263, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 370/370 [00:02<00:00, 165.92it/s, loss=1.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.0400, acc=0.5496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 370/370 [00:01<00:00, 211.51it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=0.7293, acc=0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 370/370 [00:01<00:00, 216.63it/s, loss=0.492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.5097, acc=0.8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 370/370 [00:01<00:00, 205.69it/s, loss=0.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss=0.4044, acc=0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 370/370 [00:01<00:00, 193.25it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss=0.3374, acc=0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 370/370 [00:01<00:00, 194.96it/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss=0.2932, acc=0.9320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 370/370 [00:02<00:00, 175.39it/s, loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss=0.2607, acc=0.9384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 370/370 [00:01<00:00, 193.00it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss=0.2349, acc=0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 370/370 [00:01<00:00, 194.47it/s, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss=0.2145, acc=0.9485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 370/370 [00:02<00:00, 183.08it/s, loss=0.294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss=0.1974, acc=0.9514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 370/370 [00:02<00:00, 181.13it/s, loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss=0.1839, acc=0.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 370/370 [00:01<00:00, 201.22it/s, loss=0.249] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss=0.1724, acc=0.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 370/370 [00:01<00:00, 185.34it/s, loss=0.184] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss=0.1613, acc=0.9592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 370/370 [00:01<00:00, 188.74it/s, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss=0.1523, acc=0.9612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 370/370 [00:01<00:00, 201.17it/s, loss=0.196] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss=0.1440, acc=0.9632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 370/370 [00:01<00:00, 193.59it/s, loss=0.142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss=0.1358, acc=0.9648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 370/370 [00:01<00:00, 208.96it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss=0.1313, acc=0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 370/370 [00:01<00:00, 196.01it/s, loss=0.146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss=0.1242, acc=0.9672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 370/370 [00:01<00:00, 208.40it/s, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss=0.1184, acc=0.9690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 370/370 [00:01<00:00, 192.35it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss=0.1132, acc=0.9698\n",
      "Model saved as 'mlp_ec_esm2.pt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test set: 100%|██████████| 93/93 [00:00<00:00, 245.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MLP Test Results ===\n",
      "Test accuracy MLP: 0.9756984066607498\n",
      "\n",
      "Classification report (MLP, only classes present in test/pred):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       1.1.1       0.98      0.99      0.98      1401\n",
      "       1.1.2       0.20      1.00      0.33         1\n",
      "       1.1.3       0.85      0.89      0.87        19\n",
      "       1.1.5       0.95      0.97      0.96        64\n",
      "       1.1.7       0.00      0.00      0.00         0\n",
      "       1.1.9       0.00      0.00      0.00         2\n",
      "      1.1.98       1.00      1.00      1.00         6\n",
      "      1.1.99       0.89      0.85      0.87        40\n",
      "      1.10.3       0.97      0.97      0.97       119\n",
      "      1.10.5       1.00      1.00      1.00         1\n",
      "      1.11.1       0.84      0.96      0.90       224\n",
      "      1.11.2       1.00      0.50      0.67         6\n",
      "      1.12.1       0.00      0.00      0.00         3\n",
      "      1.12.2       0.00      0.00      0.00         1\n",
      "      1.12.7       1.00      0.33      0.50         3\n",
      "     1.12.98       0.56      0.71      0.62         7\n",
      "     1.12.99       1.00      1.00      1.00         9\n",
      "     1.13.11       0.90      0.94      0.92       152\n",
      "     1.13.12       1.00      0.20      0.33         5\n",
      "     1.13.99       1.00      1.00      1.00         3\n",
      "     1.14.11       0.98      0.92      0.95        97\n",
      "     1.14.12       0.91      0.97      0.94        30\n",
      "     1.14.13       0.90      0.89      0.89        89\n",
      "     1.14.14       0.89      0.95      0.92       183\n",
      "     1.14.15       0.85      0.82      0.84        28\n",
      "     1.14.16       1.00      0.40      0.57         5\n",
      "     1.14.17       0.89      0.89      0.89         9\n",
      "     1.14.18       0.82      0.75      0.78        12\n",
      "     1.14.19       0.93      0.76      0.83        66\n",
      "     1.14.20       0.25      0.50      0.33         4\n",
      "     1.14.99       0.94      0.89      0.91        71\n",
      "      1.15.1       0.94      0.92      0.93        92\n",
      "      1.16.1       1.00      0.86      0.92         7\n",
      "      1.16.3       0.94      0.94      0.94        32\n",
      "      1.17.1       1.00      0.92      0.96       117\n",
      "      1.17.2       0.00      0.00      0.00         2\n",
      "      1.17.4       0.94      0.85      0.89        40\n",
      "      1.17.5       0.00      0.00      0.00         1\n",
      "      1.17.7       0.99      0.99      0.99       169\n",
      "      1.17.8       0.33      1.00      0.50         1\n",
      "      1.17.9       0.00      0.00      0.00         1\n",
      "     1.17.98       1.00      0.50      0.67         2\n",
      "     1.17.99       0.98      0.96      0.97        50\n",
      "      1.18.1       0.98      0.93      0.95        55\n",
      "      1.18.6       0.97      0.82      0.89        38\n",
      "       1.2.1       1.00      0.99      0.99       645\n",
      "       1.2.3       0.43      1.00      0.60         6\n",
      "       1.2.4       1.00      0.95      0.97        61\n",
      "       1.2.5       0.00      0.00      0.00         0\n",
      "       1.2.7       0.97      0.91      0.94        35\n",
      "      1.2.99       0.00      0.00      0.00         2\n",
      "      1.20.4       1.00      0.88      0.93         8\n",
      "      1.21.1       1.00      1.00      1.00         2\n",
      "      1.21.3       0.57      1.00      0.73         4\n",
      "      1.21.4       1.00      0.67      0.80         6\n",
      "     1.21.98       0.89      1.00      0.94         8\n",
      "     1.21.99       1.00      1.00      1.00         7\n",
      "      1.23.1       0.25      1.00      0.40         1\n",
      "       1.3.1       0.95      0.95      0.95       263\n",
      "       1.3.2       1.00      1.00      1.00         3\n",
      "       1.3.3       0.93      0.89      0.91        46\n",
      "       1.3.5       1.00      0.99      0.99        85\n",
      "       1.3.7       0.90      0.94      0.92        78\n",
      "       1.3.8       0.86      1.00      0.93        19\n",
      "      1.3.98       0.95      0.91      0.93        23\n",
      "      1.3.99       1.00      0.50      0.67         8\n",
      "       1.4.1       0.98      0.97      0.98        61\n",
      "       1.4.3       0.97      0.95      0.96       121\n",
      "       1.4.4       1.00      1.00      1.00       117\n",
      "       1.4.7       0.50      0.67      0.57         3\n",
      "       1.4.9       1.00      1.00      1.00         1\n",
      "      1.4.99       0.00      0.00      0.00         1\n",
      "       1.5.1       1.00      0.96      0.98       242\n",
      "       1.5.3       1.00      0.33      0.50         9\n",
      "       1.5.5       1.00      1.00      1.00         5\n",
      "       1.5.7       0.00      0.00      0.00         1\n",
      "       1.5.8       1.00      1.00      1.00         1\n",
      "      1.5.98       0.90      0.82      0.86        11\n",
      "      1.5.99       0.60      0.75      0.67         4\n",
      "       1.6.1       1.00      1.00      1.00        16\n",
      "       1.6.2       1.00      1.00      1.00        20\n",
      "       1.6.3       0.67      0.80      0.73         5\n",
      "       1.6.5       0.97      0.97      0.97        66\n",
      "      1.6.99       1.00      0.67      0.80         9\n",
      "       1.7.1       0.95      0.97      0.96       188\n",
      "       1.7.2       0.97      0.97      0.97        35\n",
      "       1.7.3       1.00      1.00      1.00         3\n",
      "       1.7.5       0.50      1.00      0.67         3\n",
      "       1.7.7       1.00      0.67      0.80         3\n",
      "      1.7.99       1.00      1.00      1.00        30\n",
      "       1.8.1       0.89      0.96      0.92       113\n",
      "       1.8.3       0.93      0.93      0.93        15\n",
      "       1.8.4       0.94      0.99      0.97       141\n",
      "       1.8.5       1.00      0.44      0.62         9\n",
      "       1.8.7       1.00      0.50      0.67        10\n",
      "      1.8.98       1.00      0.88      0.93         8\n",
      "      1.8.99       0.00      0.00      0.00         0\n",
      "       1.9.6       0.91      1.00      0.95        30\n",
      "      1.97.1       0.99      0.97      0.98        99\n",
      "       2.1.1       0.99      0.99      0.99      2293\n",
      "       2.1.2       1.00      1.00      1.00       590\n",
      "       2.1.3       1.00      1.00      1.00       424\n",
      "       2.1.4       1.00      1.00      1.00         3\n",
      "       2.1.5       0.00      0.00      0.00         0\n",
      "      2.10.1       1.00      1.00      1.00        12\n",
      "       2.2.1       1.00      0.99      0.99       283\n",
      "       2.3.1       0.99      0.98      0.99      1778\n",
      "       2.3.2       0.93      0.99      0.96       531\n",
      "       2.3.3       0.96      0.96      0.96       171\n",
      "       2.4.1       0.98      0.99      0.98       733\n",
      "       2.4.2       1.00      0.98      0.99      1022\n",
      "       2.4.3       1.00      0.93      0.96        14\n",
      "      2.4.99       0.99      1.00      1.00       186\n",
      "       2.5.1       0.98      0.97      0.98      1572\n",
      "       2.6.1       0.98      0.99      0.98       358\n",
      "      2.6.99       1.00      1.00      1.00        51\n",
      "       2.7.1       0.99      0.97      0.98      1582\n",
      "      2.7.10       0.99      0.86      0.92       128\n",
      "      2.7.11       0.95      0.98      0.97       839\n",
      "      2.7.12       1.00      0.59      0.74        34\n",
      "      2.7.13       0.96      0.98      0.97       108\n",
      "      2.7.14       1.00      1.00      1.00        14\n",
      "       2.7.2       1.00      1.00      1.00       415\n",
      "       2.7.3       0.92      0.96      0.94        25\n",
      "       2.7.4       0.99      0.99      0.99       788\n",
      "       2.7.6       0.93      0.99      0.96        70\n",
      "       2.7.7       0.97      0.99      0.98      2462\n",
      "       2.7.8       0.99      1.00      1.00       391\n",
      "       2.7.9       1.00      1.00      1.00        28\n",
      "       2.8.1       1.00      1.00      1.00       652\n",
      "       2.8.2       1.00      1.00      1.00        28\n",
      "       2.8.3       0.92      1.00      0.96        22\n",
      "       2.8.4       1.00      0.99      1.00       217\n",
      "       2.8.5       1.00      1.00      1.00         3\n",
      "       2.9.1       0.97      0.98      0.97        57\n",
      "       3.1.1       0.95      0.96      0.96       758\n",
      "      3.1.11       1.00      0.99      1.00       192\n",
      "      3.1.12       1.00      0.83      0.91         6\n",
      "      3.1.13       1.00      1.00      1.00        41\n",
      "      3.1.16       1.00      1.00      1.00         1\n",
      "       3.1.2       0.96      0.98      0.97        95\n",
      "      3.1.21       0.99      0.96      0.97       249\n",
      "      3.1.22       0.00      0.00      0.00         1\n",
      "      3.1.26       0.98      0.98      0.98       524\n",
      "       3.1.3       0.98      0.98      0.98       676\n",
      "      3.1.30       0.50      1.00      0.67         1\n",
      "      3.1.31       1.00      1.00      1.00         2\n",
      "       3.1.4       0.92      0.89      0.90        90\n",
      "       3.1.5       1.00      1.00      1.00         6\n",
      "       3.1.6       1.00      0.92      0.96        13\n",
      "       3.1.7       1.00      0.25      0.40         8\n",
      "       3.1.8       0.00      0.00      0.00         0\n",
      "      3.11.1       1.00      1.00      1.00        15\n",
      "      3.13.1       1.00      0.33      0.50         3\n",
      "      3.13.2       0.93      0.98      0.95        43\n",
      "       3.2.1       0.91      0.96      0.93       669\n",
      "       3.2.2       0.99      0.95      0.97       286\n",
      "       3.3.2       0.91      0.91      0.91        23\n",
      "      3.4.11       0.99      0.98      0.98       228\n",
      "      3.4.13       1.00      0.96      0.98        49\n",
      "      3.4.14       1.00      0.94      0.97        33\n",
      "      3.4.15       1.00      0.80      0.89         5\n",
      "      3.4.16       0.96      1.00      0.98        43\n",
      "      3.4.17       0.91      0.95      0.93        21\n",
      "      3.4.19       0.96      0.94      0.95       157\n",
      "      3.4.21       1.00      0.98      0.99       444\n",
      "      3.4.22       0.96      0.81      0.88        67\n",
      "      3.4.23       0.98      0.99      0.99       131\n",
      "      3.4.24       0.97      0.96      0.97        72\n",
      "      3.4.25       1.00      0.99      1.00       139\n",
      "       3.5.1       0.97      0.98      0.98       887\n",
      "       3.5.2       0.99      0.99      0.99       239\n",
      "       3.5.3       0.92      0.99      0.95       137\n",
      "       3.5.4       0.92      0.97      0.94       516\n",
      "       3.5.5       0.92      0.85      0.88        13\n",
      "      3.5.99       0.96      0.99      0.97        72\n",
      "       3.6.1       0.98      0.97      0.98       799\n",
      "       3.6.4       0.96      0.98      0.97       384\n",
      "       3.6.5       1.00      1.00      1.00       344\n",
      "       3.7.1       0.97      0.95      0.96        37\n",
      "       3.8.1       1.00      0.81      0.90        16\n",
      "       4.1.1       0.97      0.97      0.97      1040\n",
      "       4.1.2       0.96      0.95      0.96       198\n",
      "       4.1.3       0.95      0.96      0.95       148\n",
      "      4.1.99       0.97      0.96      0.97       217\n",
      "       4.2.1       0.98      0.97      0.98      1612\n",
      "       4.2.2       0.94      1.00      0.97        48\n",
      "       4.2.3       0.97      0.99      0.98       415\n",
      "      4.2.99       0.93      0.96      0.95        54\n",
      "       4.3.1       0.88      0.93      0.91       118\n",
      "       4.3.2       0.97      0.98      0.97       312\n",
      "       4.3.3       1.00      0.99      1.00       136\n",
      "      4.3.99       1.00      1.00      1.00         7\n",
      "       4.4.1       0.87      0.90      0.88       118\n",
      "       4.6.1       0.94      0.93      0.94       275\n",
      "       4.8.1       1.00      0.50      0.67         2\n",
      "      4.98.1       0.97      1.00      0.99        75\n",
      "      4.99.1       1.00      0.83      0.91        35\n",
      "       5.1.1       0.99      0.97      0.98       248\n",
      "       5.1.2       0.00      0.00      0.00         1\n",
      "       5.1.3       0.96      0.95      0.96       155\n",
      "      5.1.99       0.96      1.00      0.98        22\n",
      "       5.2.1       0.98      0.96      0.97       307\n",
      "       5.3.1       0.99      1.00      0.99       825\n",
      "       5.3.2       1.00      0.88      0.93        16\n",
      "       5.3.3       0.98      0.91      0.95        69\n",
      "       5.3.4       1.00      0.92      0.96        24\n",
      "      5.3.99       1.00      0.71      0.83         7\n",
      "       5.4.2       1.00      1.00      1.00       390\n",
      "       5.4.3       1.00      1.00      1.00       147\n",
      "       5.4.4       1.00      1.00      1.00         3\n",
      "      5.4.99       0.98      0.95      0.97       386\n",
      "       5.5.1       0.84      0.88      0.86        43\n",
      "       5.6.1       0.99      1.00      0.99       219\n",
      "       5.6.2       0.98      0.94      0.96       212\n",
      "      5.99.1       0.00      0.00      0.00         0\n",
      "       6.1.1       1.00      1.00      1.00      2576\n",
      "       6.2.1       0.98      0.99      0.98       277\n",
      "       6.3.1       0.98      0.97      0.98       231\n",
      "       6.3.2       0.99      0.99      0.99       716\n",
      "       6.3.3       1.00      0.99      1.00       138\n",
      "       6.3.4       0.98      0.99      0.98       788\n",
      "       6.3.5       0.99      1.00      0.99       400\n",
      "       6.4.1       1.00      0.90      0.95        10\n",
      "       6.5.1       0.98      0.99      0.98       253\n",
      "       6.6.1       1.00      0.85      0.92        13\n",
      "       6.7.1       0.00      0.00      0.00         2\n",
      "       7.1.1       0.99      0.96      0.98       464\n",
      "       7.1.2       0.99      0.99      0.99       397\n",
      "       7.1.3       0.67      0.50      0.57         4\n",
      "       7.2.1       0.95      0.99      0.97       102\n",
      "       7.2.2       1.00      0.92      0.96       108\n",
      "       7.2.3       0.83      0.83      0.83         6\n",
      "       7.2.4       1.00      0.93      0.97        15\n",
      "       7.3.2       0.95      0.98      0.96       127\n",
      "       7.4.2       1.00      1.00      1.00       217\n",
      "       7.5.2       0.99      0.97      0.98        79\n",
      "       7.6.2       0.95      0.98      0.96       139\n",
      "\n",
      "    accuracy                           0.98     47322\n",
      "   macro avg       0.87      0.84      0.84     47322\n",
      "weighted avg       0.98      0.98      0.98     47322\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================\n",
    "#  MLP for EC prediction using ESM2 pooled embeddings\n",
    "#  - Train + Test + Report\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Load features & labels\n",
    "# -----------------------------------------------------\n",
    "X = np.load(\"esm2_features.npy\")  # shape should be (236607, 480)\n",
    "print(\"Shape X:\", X.shape)\n",
    "\n",
    "df = pd.read_csv(\"3_levels_EC.tsv\", sep=\"\\t\")\n",
    "labels = df[\"EC number\"].astype(str).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Total classes:\", num_classes)\n",
    "print(\"Example classes:\", le.classes_[:20])\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Train-test split (same style as RF / XGB)\n",
    "# -----------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=None   # don't stratify because there are very rare classes\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size :\", X_test.shape[0])\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Convert to PyTorch tensors & DataLoader\n",
    "# -----------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 512  # can be increased if VRAM is sufficient\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Define MLP model (adjust as needed)\n",
    "# -----------------------------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=480, hidden_dim=512, num_classes=263, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP(input_dim=X.shape[1], hidden_dim=512, num_classes=num_classes, dropout=0.3).to(device)\n",
    "print(model)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. Loss, optimizer, training config\n",
    "# -----------------------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 20\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 6. Training loop with progress bar\n",
    "# -----------------------------------------------------\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "    for xb, yb in pbar:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch}: loss={epoch_loss:.4f}, acc={epoch_acc:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 7. Save trained model\n",
    "# -----------------------------------------------------\n",
    "torch.save(model.state_dict(), \"mlp_ec_esm2.pt\")\n",
    "print(\"Model saved as 'mlp_ec_esm2.pt'\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 8. Evaluation on test set\n",
    "# -----------------------------------------------------\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(test_loader, desc=\"Evaluating on test set\"):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(yb.cpu().numpy())\n",
    "\n",
    "y_true = np.concatenate(all_targets)\n",
    "y_pred = np.concatenate(all_preds)\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "print(\"\\n=== MLP Test Results ===\")\n",
    "print(\"Test accuracy MLP:\", test_acc)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 9. Classification report (avoid class mismatch errors)\n",
    "# -----------------------------------------------------\n",
    "labels_used = np.unique(np.concatenate([y_true, y_pred]))\n",
    "target_names = le.inverse_transform(labels_used)\n",
    "\n",
    "print(\"\\nClassification report (MLP, only classes present in test/pred):\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=labels_used,\n",
    "    target_names=target_names,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d5e243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9340bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Downloads\\AI FINAL PROJECT TESTING\\.venv311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Num classes: 263\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "label_encoder = joblib.load(\"label_encoder_ec_esm2.joblib\")\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Num classes:\", num_classes)\n",
    "\n",
    "class MLP_EC(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "\n",
    "            torch.nn.Linear(512, 512),     # hidden 2 (512 -> 512)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "\n",
    "            torch.nn.Linear(512, num_classes),  # output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08f4a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIM: 480\n",
      "MLP model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_31832\\1030863462.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"mlp_ec_esm2.pt\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"esm2_features.npy\")  \n",
    "INPUT_DIM = features.shape[1]\n",
    "print(\"INPUT_DIM:\", INPUT_DIM)\n",
    "\n",
    "model = MLP_EC(INPUT_DIM, num_classes).to(device)\n",
    "\n",
    "state_dict = torch.load(\"mlp_ec_esm2.pt\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "print(\"MLP model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f840a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ESM2 model: facebook/esm2_t12_35M_UR50D\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"facebook/esm2_t12_35M_UR50D\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "esm_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "esm_model.to(device)\n",
    "esm_model.eval()\n",
    "\n",
    "print(\"Loaded ESM2 model:\", MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b785ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def embed_seq(seq: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed 1 protein sequence using ESM2 t12 mean pooling.\n",
    "    seq: amino acid string (A,C,D,...), without spaces.\n",
    "    return: numpy array shape (INPUT_DIM,)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(\n",
    "        [seq],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "    outputs = esm_model(**tokens)           # last_hidden_state: [1, L, D]\n",
    "    last_hidden = outputs.last_hidden_state # [1, L, D]\n",
    "    mask = tokens[\"attention_mask\"].unsqueeze(-1)  # [1, L, 1]\n",
    "\n",
    "    masked = last_hidden * mask             # [1, L, D]\n",
    "    summed = masked.sum(dim=1)              # [1, D]\n",
    "    counts = mask.sum(dim=1)                # [1, 1]\n",
    "    emb = (summed / counts).cpu().numpy()[0]    # [D]\n",
    "\n",
    "    # check dimension\n",
    "    if emb.shape[0] != INPUT_DIM:\n",
    "        raise ValueError(\n",
    "            f\"Embedding dimension ({emb.shape[0]}) != INPUT_DIM MLP ({INPUT_DIM}). \"\n",
    "            \"This means MODEL_NAME used is different from preprocessing.\"\n",
    "        )\n",
    "\n",
    "    return emb\n",
    "@torch.no_grad()\n",
    "def predict_ec(seq: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Predict EC number from one protein sequence.\n",
    "    seq  : amino acid string (A,C,D,...)\n",
    "    top_k: show top N predictions\n",
    "    \"\"\"\n",
    "    emb = embed_seq(seq)                                      # (D,)\n",
    "    x = torch.tensor(emb, dtype=torch.float32, device=device).unsqueeze(0)  # (1, D)\n",
    "\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)[0].cpu().numpy()     # (num_classes,)\n",
    "\n",
    "    top_idx = probs.argsort()[::-1][:top_k]\n",
    "    top_ec  = label_encoder.inverse_transform(top_idx)\n",
    "    top_conf = probs[top_idx]\n",
    "\n",
    "    print(\"Top predictions:\")\n",
    "    for ec, p in zip(top_ec, top_conf):\n",
    "        print(f\"  {ec:10s}  (confidence = {p:.4f})\")\n",
    "\n",
    "    return top_ec[0], top_conf[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb252ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictions:\n",
      "  2.7.1       (confidence = 0.7490)\n",
      "  2.3.1       (confidence = 0.1324)\n",
      "  3.1.1       (confidence = 0.0702)\n",
      "\n",
      "Final prediction:\n",
      "Predicted EC : 2.7.1\n",
      "Confidence   : 0.74900246\n"
     ]
    }
   ],
   "source": [
    "TEST_SEQ = input(\"Input sequence protein (A,C,D,... without space): \").strip()\n",
    "pred_ec, conf = predict_ec(TEST_SEQ, top_k=3)\n",
    "\n",
    "print(\"\\nFinal prediction:\")\n",
    "print(\"Predicted EC :\", pred_ec)\n",
    "print(\"Confidence   :\", conf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Deepfake GPU)",
   "language": "python",
   "name": "deepfake-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
