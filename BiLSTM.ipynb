{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d652d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Downloads\\AI FINAL PROJECT TESTING\\.venv311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (236607, 480)\n",
      "Total classes: 263\n",
      "Example classes: ['1.1.1' '1.1.2' '1.1.3' '1.1.5' '1.1.7' '1.1.9' '1.1.98' '1.1.99'\n",
      " '1.10.3' '1.10.5' '1.11.1' '1.11.2' '1.12.1' '1.12.2' '1.12.5' '1.12.7'\n",
      " '1.12.98' '1.12.99' '1.13.11' '1.13.12']\n",
      "Train size: 189285\n",
      "Test size : 47322\n",
      "Using device: cuda\n",
      "BiLSTMClassifier(\n",
      "  (lstm): LSTM(32, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=263, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 740/740 [00:03<00:00, 185.06it/s, loss=1.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.9993, acc=0.3332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 740/740 [00:03<00:00, 188.08it/s, loss=1.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=1.0446, acc=0.7669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 740/740 [00:03<00:00, 189.65it/s, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.6391, acc=0.8547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 740/740 [00:04<00:00, 176.68it/s, loss=0.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss=0.4688, acc=0.8921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 740/740 [00:04<00:00, 182.22it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss=0.3680, acc=0.9148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 740/740 [00:03<00:00, 186.51it/s, loss=0.246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss=0.3041, acc=0.9277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 740/740 [00:04<00:00, 174.79it/s, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss=0.2562, acc=0.9384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 740/740 [00:04<00:00, 177.59it/s, loss=0.333] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss=0.2269, acc=0.9446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 740/740 [00:04<00:00, 178.49it/s, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss=0.1998, acc=0.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 740/740 [00:03<00:00, 200.44it/s, loss=0.177] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss=0.1750, acc=0.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 740/740 [00:04<00:00, 180.27it/s, loss=0.148] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss=0.1575, acc=0.9601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 740/740 [00:03<00:00, 203.34it/s, loss=0.0828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss=0.1420, acc=0.9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 740/740 [00:03<00:00, 200.59it/s, loss=0.0781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss=0.1275, acc=0.9672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 740/740 [00:03<00:00, 198.22it/s, loss=0.0991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss=0.1167, acc=0.9693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 740/740 [00:03<00:00, 189.35it/s, loss=0.155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss=0.1068, acc=0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 740/740 [00:03<00:00, 196.03it/s, loss=0.0673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss=0.0972, acc=0.9739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 740/740 [00:04<00:00, 179.05it/s, loss=0.144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss=0.0858, acc=0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 740/740 [00:03<00:00, 198.67it/s, loss=0.0719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss=0.0835, acc=0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 740/740 [00:03<00:00, 200.03it/s, loss=0.0531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss=0.0757, acc=0.9790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 740/740 [00:03<00:00, 194.78it/s, loss=0.126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss=0.0738, acc=0.9795\n",
      "Model saved as 'bilstm_ec_esm2.pt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (BiLSTM): 100%|██████████| 185/185 [00:00<00:00, 337.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BiLSTM Test Results ===\n",
      "Test Accuracy (BiLSTM): 0.9612865052195596\n",
      "\n",
      "Classification report (BiLSTM, only classes present in test/pred):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       1.1.1       0.98      0.98      0.98      1401\n",
      "       1.1.2       0.17      1.00      0.29         1\n",
      "       1.1.3       0.72      0.95      0.82        19\n",
      "       1.1.5       0.94      0.95      0.95        64\n",
      "       1.1.9       0.00      0.00      0.00         2\n",
      "      1.1.98       0.86      1.00      0.92         6\n",
      "      1.1.99       0.92      0.82      0.87        40\n",
      "      1.10.3       0.97      0.97      0.97       119\n",
      "      1.10.5       1.00      1.00      1.00         1\n",
      "      1.11.1       0.85      0.92      0.88       224\n",
      "      1.11.2       1.00      0.17      0.29         6\n",
      "      1.12.1       1.00      0.33      0.50         3\n",
      "      1.12.2       0.00      0.00      0.00         1\n",
      "      1.12.7       1.00      0.67      0.80         3\n",
      "     1.12.98       1.00      0.43      0.60         7\n",
      "     1.12.99       1.00      1.00      1.00         9\n",
      "     1.13.11       0.94      0.96      0.95       152\n",
      "     1.13.12       1.00      0.20      0.33         5\n",
      "     1.13.99       1.00      1.00      1.00         3\n",
      "     1.14.11       0.94      0.94      0.94        97\n",
      "     1.14.12       0.93      0.90      0.92        30\n",
      "     1.14.13       0.79      0.85      0.82        89\n",
      "     1.14.14       0.87      0.94      0.90       183\n",
      "     1.14.15       0.90      0.68      0.78        28\n",
      "     1.14.16       0.20      0.20      0.20         5\n",
      "     1.14.17       1.00      1.00      1.00         9\n",
      "     1.14.18       0.75      0.75      0.75        12\n",
      "     1.14.19       0.95      0.61      0.74        66\n",
      "     1.14.20       1.00      0.50      0.67         4\n",
      "     1.14.99       0.92      0.93      0.92        71\n",
      "      1.15.1       0.87      0.86      0.86        92\n",
      "      1.16.1       0.83      0.71      0.77         7\n",
      "      1.16.3       0.91      0.91      0.91        32\n",
      "      1.17.1       0.94      0.96      0.95       117\n",
      "      1.17.2       0.00      0.00      0.00         2\n",
      "      1.17.4       0.94      0.85      0.89        40\n",
      "      1.17.5       0.00      0.00      0.00         1\n",
      "      1.17.7       0.94      0.99      0.97       169\n",
      "      1.17.8       1.00      1.00      1.00         1\n",
      "      1.17.9       0.00      0.00      0.00         1\n",
      "     1.17.98       1.00      0.50      0.67         2\n",
      "     1.17.99       1.00      0.94      0.97        50\n",
      "      1.18.1       0.88      0.95      0.91        55\n",
      "      1.18.6       1.00      0.82      0.90        38\n",
      "       1.2.1       0.99      0.98      0.98       645\n",
      "       1.2.3       0.00      0.00      0.00         6\n",
      "       1.2.4       1.00      0.92      0.96        61\n",
      "       1.2.5       0.00      0.00      0.00         0\n",
      "       1.2.7       0.79      0.86      0.82        35\n",
      "      1.2.99       0.00      0.00      0.00         2\n",
      "      1.20.4       0.89      1.00      0.94         8\n",
      "      1.21.1       1.00      1.00      1.00         2\n",
      "      1.21.3       0.67      1.00      0.80         4\n",
      "      1.21.4       1.00      0.67      0.80         6\n",
      "     1.21.98       0.89      1.00      0.94         8\n",
      "     1.21.99       1.00      0.86      0.92         7\n",
      "      1.23.1       0.20      1.00      0.33         1\n",
      "       1.3.1       0.95      0.94      0.95       263\n",
      "       1.3.2       1.00      0.33      0.50         3\n",
      "       1.3.3       0.98      0.87      0.92        46\n",
      "       1.3.5       1.00      1.00      1.00        85\n",
      "       1.3.7       0.90      0.90      0.90        78\n",
      "       1.3.8       0.90      1.00      0.95        19\n",
      "      1.3.98       1.00      0.91      0.95        23\n",
      "      1.3.99       0.27      0.38      0.32         8\n",
      "       1.4.1       0.92      0.90      0.91        61\n",
      "       1.4.3       0.97      0.93      0.95       121\n",
      "       1.4.4       1.00      1.00      1.00       117\n",
      "       1.4.7       1.00      0.33      0.50         3\n",
      "       1.4.9       1.00      1.00      1.00         1\n",
      "      1.4.99       0.00      0.00      0.00         1\n",
      "       1.5.1       0.97      0.94      0.96       242\n",
      "       1.5.3       1.00      0.44      0.62         9\n",
      "       1.5.5       1.00      1.00      1.00         5\n",
      "       1.5.7       0.00      0.00      0.00         1\n",
      "       1.5.8       0.50      1.00      0.67         1\n",
      "      1.5.98       0.88      0.64      0.74        11\n",
      "      1.5.99       0.50      0.25      0.33         4\n",
      "       1.6.1       1.00      1.00      1.00        16\n",
      "       1.6.2       1.00      1.00      1.00        20\n",
      "       1.6.3       0.80      0.80      0.80         5\n",
      "       1.6.5       0.95      0.95      0.95        66\n",
      "      1.6.99       1.00      0.67      0.80         9\n",
      "       1.7.1       0.92      0.96      0.94       188\n",
      "       1.7.2       0.92      0.94      0.93        35\n",
      "       1.7.3       0.75      1.00      0.86         3\n",
      "       1.7.5       0.75      1.00      0.86         3\n",
      "       1.7.7       1.00      0.67      0.80         3\n",
      "      1.7.99       1.00      1.00      1.00        30\n",
      "       1.8.1       0.96      0.95      0.95       113\n",
      "       1.8.3       1.00      0.80      0.89        15\n",
      "       1.8.4       0.89      0.93      0.91       141\n",
      "       1.8.5       1.00      0.44      0.62         9\n",
      "       1.8.7       0.89      0.80      0.84        10\n",
      "      1.8.98       1.00      0.62      0.77         8\n",
      "       1.9.6       0.97      1.00      0.98        30\n",
      "      1.97.1       0.96      0.95      0.95        99\n",
      "       2.1.1       0.98      0.97      0.98      2293\n",
      "       2.1.2       0.99      1.00      1.00       590\n",
      "       2.1.3       0.99      1.00      1.00       424\n",
      "       2.1.4       1.00      1.00      1.00         3\n",
      "       2.1.5       0.00      0.00      0.00         0\n",
      "      2.10.1       1.00      1.00      1.00        12\n",
      "       2.2.1       1.00      0.96      0.98       283\n",
      "       2.3.1       0.99      0.97      0.98      1778\n",
      "       2.3.2       0.92      0.93      0.93       531\n",
      "       2.3.3       0.93      0.96      0.94       171\n",
      "       2.4.1       0.95      0.97      0.96       733\n",
      "       2.4.2       0.98      0.98      0.98      1022\n",
      "       2.4.3       0.93      0.93      0.93        14\n",
      "      2.4.99       0.98      0.99      0.99       186\n",
      "       2.5.1       0.96      0.96      0.96      1572\n",
      "       2.6.1       0.97      0.98      0.98       358\n",
      "      2.6.99       1.00      1.00      1.00        51\n",
      "       2.7.1       0.98      0.96      0.97      1582\n",
      "      2.7.10       0.99      0.82      0.90       128\n",
      "      2.7.11       0.95      0.96      0.95       839\n",
      "      2.7.12       0.89      0.47      0.62        34\n",
      "      2.7.13       0.82      0.95      0.88       108\n",
      "      2.7.14       0.93      1.00      0.97        14\n",
      "       2.7.2       0.99      1.00      0.99       415\n",
      "       2.7.3       0.81      0.88      0.85        25\n",
      "       2.7.4       0.98      0.97      0.98       788\n",
      "       2.7.6       0.93      0.97      0.95        70\n",
      "       2.7.7       0.95      0.98      0.96      2462\n",
      "       2.7.8       0.99      0.99      0.99       391\n",
      "       2.7.9       1.00      1.00      1.00        28\n",
      "       2.8.1       0.99      1.00      0.99       652\n",
      "       2.8.2       0.97      1.00      0.98        28\n",
      "       2.8.3       0.88      1.00      0.94        22\n",
      "       2.8.4       0.99      0.99      0.99       217\n",
      "       2.8.5       1.00      1.00      1.00         3\n",
      "       2.9.1       1.00      0.95      0.97        57\n",
      "       3.1.1       0.93      0.94      0.93       758\n",
      "      3.1.11       0.99      0.99      0.99       192\n",
      "      3.1.12       1.00      0.67      0.80         6\n",
      "      3.1.13       0.95      1.00      0.98        41\n",
      "      3.1.16       0.50      1.00      0.67         1\n",
      "       3.1.2       0.95      0.96      0.95        95\n",
      "      3.1.21       0.96      0.95      0.95       249\n",
      "      3.1.22       0.00      0.00      0.00         1\n",
      "      3.1.26       0.98      0.98      0.98       524\n",
      "       3.1.3       0.94      0.94      0.94       676\n",
      "      3.1.30       1.00      1.00      1.00         1\n",
      "      3.1.31       1.00      1.00      1.00         2\n",
      "       3.1.4       0.83      0.78      0.80        90\n",
      "       3.1.5       1.00      1.00      1.00         6\n",
      "       3.1.6       1.00      0.85      0.92        13\n",
      "       3.1.7       1.00      0.12      0.22         8\n",
      "      3.11.1       1.00      1.00      1.00        15\n",
      "      3.13.1       0.00      0.00      0.00         3\n",
      "      3.13.2       0.84      0.98      0.90        43\n",
      "       3.2.1       0.89      0.96      0.92       669\n",
      "       3.2.2       0.94      0.94      0.94       286\n",
      "       3.3.2       0.86      0.83      0.84        23\n",
      "      3.4.11       0.97      0.98      0.97       228\n",
      "      3.4.13       0.98      0.94      0.96        49\n",
      "      3.4.14       0.94      0.91      0.92        33\n",
      "      3.4.15       1.00      0.80      0.89         5\n",
      "      3.4.16       0.93      0.98      0.95        43\n",
      "      3.4.17       0.86      0.90      0.88        21\n",
      "      3.4.19       0.91      0.95      0.93       157\n",
      "      3.4.21       0.99      0.97      0.98       444\n",
      "      3.4.22       0.91      0.78      0.84        67\n",
      "      3.4.23       0.96      0.98      0.97       131\n",
      "      3.4.24       0.90      0.97      0.93        72\n",
      "      3.4.25       0.98      0.94      0.96       139\n",
      "       3.5.1       0.93      0.97      0.95       887\n",
      "       3.5.2       1.00      0.97      0.98       239\n",
      "       3.5.3       0.79      0.93      0.85       137\n",
      "       3.5.4       0.92      0.93      0.92       516\n",
      "       3.5.5       1.00      0.62      0.76        13\n",
      "      3.5.99       0.96      0.96      0.96        72\n",
      "       3.6.1       0.96      0.98      0.97       799\n",
      "       3.6.4       0.97      0.95      0.96       384\n",
      "       3.6.5       0.99      1.00      0.99       344\n",
      "       3.7.1       0.97      0.92      0.94        37\n",
      "       3.8.1       1.00      0.62      0.77        16\n",
      "       4.1.1       0.95      0.96      0.96      1040\n",
      "       4.1.2       0.98      0.94      0.96       198\n",
      "       4.1.3       0.92      0.91      0.91       148\n",
      "      4.1.99       0.99      0.95      0.97       217\n",
      "       4.2.1       0.96      0.96      0.96      1612\n",
      "       4.2.2       1.00      0.83      0.91        48\n",
      "       4.2.3       0.98      0.97      0.97       415\n",
      "      4.2.99       0.89      0.93      0.91        54\n",
      "       4.3.1       0.87      0.92      0.89       118\n",
      "       4.3.2       0.99      0.93      0.96       312\n",
      "       4.3.3       0.99      0.96      0.98       136\n",
      "      4.3.99       0.88      1.00      0.93         7\n",
      "       4.4.1       0.82      0.84      0.83       118\n",
      "       4.6.1       0.90      0.89      0.90       275\n",
      "       4.8.1       1.00      0.50      0.67         2\n",
      "      4.98.1       0.96      0.99      0.97        75\n",
      "      4.99.1       0.94      0.83      0.88        35\n",
      "       5.1.1       1.00      0.97      0.98       248\n",
      "       5.1.2       0.00      0.00      0.00         1\n",
      "       5.1.3       0.94      0.97      0.96       155\n",
      "      5.1.99       1.00      1.00      1.00        22\n",
      "       5.2.1       0.95      0.95      0.95       307\n",
      "       5.3.1       0.99      0.99      0.99       825\n",
      "       5.3.2       0.88      0.88      0.88        16\n",
      "       5.3.3       1.00      0.91      0.95        69\n",
      "       5.3.4       1.00      0.92      0.96        24\n",
      "      5.3.99       0.75      0.86      0.80         7\n",
      "       5.4.2       0.99      0.99      0.99       390\n",
      "       5.4.3       1.00      1.00      1.00       147\n",
      "       5.4.4       0.60      1.00      0.75         3\n",
      "      5.4.99       0.97      0.90      0.93       386\n",
      "       5.5.1       0.82      0.77      0.80        43\n",
      "       5.6.1       0.96      0.99      0.98       219\n",
      "       5.6.2       0.94      0.92      0.93       212\n",
      "      5.99.1       0.00      0.00      0.00         0\n",
      "       6.1.1       1.00      1.00      1.00      2576\n",
      "       6.1.3       0.00      0.00      0.00         0\n",
      "       6.2.1       0.95      0.97      0.96       277\n",
      "       6.3.1       0.98      0.97      0.97       231\n",
      "       6.3.2       0.97      0.98      0.98       716\n",
      "       6.3.3       0.99      0.97      0.98       138\n",
      "       6.3.4       0.97      0.95      0.96       788\n",
      "       6.3.5       0.99      1.00      0.99       400\n",
      "       6.4.1       1.00      0.60      0.75        10\n",
      "       6.5.1       0.97      0.96      0.97       253\n",
      "       6.6.1       1.00      0.92      0.96        13\n",
      "       6.7.1       0.00      0.00      0.00         2\n",
      "       7.1.1       0.98      0.97      0.97       464\n",
      "       7.1.2       0.99      0.98      0.98       397\n",
      "       7.1.3       0.38      0.75      0.50         4\n",
      "       7.2.1       0.96      0.94      0.95       102\n",
      "       7.2.2       0.99      0.92      0.95       108\n",
      "       7.2.3       1.00      0.17      0.29         6\n",
      "       7.2.4       1.00      1.00      1.00        15\n",
      "       7.3.2       0.95      0.97      0.96       127\n",
      "       7.4.2       0.98      0.99      0.98       217\n",
      "       7.5.2       1.00      0.97      0.99        79\n",
      "       7.6.2       0.97      0.98      0.97       139\n",
      "\n",
      "    accuracy                           0.96     47322\n",
      "   macro avg       0.86      0.82      0.82     47322\n",
      "weighted avg       0.96      0.96      0.96     47322\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =====================================================\n",
    "# 1. Load ESM2 embeddings & labels\n",
    "# =====================================================\n",
    "X = np.load(\"esm2_features.npy\")  # shape = (236607, 480)\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "df = pd.read_csv(\"3_levels_EC.tsv\", sep=\"\\t\")\n",
    "labels = df[\"EC number\"].astype(str).values\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Total classes:\", num_classes)\n",
    "print(\"Example classes:\", le.classes_[:20])\n",
    "\n",
    "# =====================================================\n",
    "# 2. Train-test split\n",
    "# =====================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=None  # jangan stratify karena ada kelas sangat langka\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size :\", X_test.shape[0])\n",
    "\n",
    "# =====================================================\n",
    "# 3. Dataset & DataLoader (reshape ke pseudo-sequence)\n",
    "# =====================================================\n",
    "SEQ_LEN = 15\n",
    "EMB_DIM = 32  # 15 × 32 = 480\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # reshape (N, 480) -> (N, 15, 32)\n",
    "        X = X.reshape(len(X), SEQ_LEN, EMB_DIM)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = ProteinDataset(X_train, y_train)\n",
    "test_ds  = ProteinDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "# =====================================================\n",
    "# 4. BiLSTM model\n",
    "# =====================================================\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim=32, hidden_size=128, num_layers=2, num_classes=263):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # karena bidirectional -> hidden_size * 2\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len=15, embed_dim=32)\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        # hn shape: (num_layers*2, batch, hidden_size)\n",
    "        # pakai last layer, dua arah: hn[-2] (forward), hn[-1] (backward)\n",
    "        final = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1)  # (batch, hidden_size*2)\n",
    "        logits = self.fc(final)\n",
    "        return logits\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = BiLSTMClassifier(\n",
    "    embed_dim=EMB_DIM,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# =====================================================\n",
    "# 5. Optimizer & Loss\n",
    "# =====================================================\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "# =====================================================\n",
    "# 6. Training loop with tqdm\n",
    "# =====================================================\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for Xb, yb in pbar:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch {epoch}: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# 7. Save model\n",
    "# =====================================================\n",
    "torch.save(model.state_dict(), \"bilstm_ec_esm2.pt\")\n",
    "print(\"Model saved as 'bilstm_ec_esm2.pt'\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. Evaluation: test accuracy + classification_report\n",
    "# =====================================================\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in tqdm(test_loader, desc=\"Evaluating (BiLSTM)\"):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        logits = model(Xb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(yb.cpu().numpy())\n",
    "\n",
    "y_true = np.concatenate(all_targets)\n",
    "y_pred = np.concatenate(all_preds)\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "print(\"\\n=== BiLSTM Test Results ===\")\n",
    "print(\"Test Accuracy (BiLSTM):\", test_acc)\n",
    "\n",
    "# Hanya kelas yang muncul di y_true/y_pred supaya tidak error\n",
    "labels_used = np.unique(np.concatenate([y_true, y_pred]))\n",
    "target_names = le.inverse_transform(labels_used)\n",
    "\n",
    "print(\"\\nClassification report (BiLSTM, only classes present in test/pred):\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=labels_used,\n",
    "    target_names=target_names,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Deepfake GPU)",
   "language": "python",
   "name": "deepfake-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
