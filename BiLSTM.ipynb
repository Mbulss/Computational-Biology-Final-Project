{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d652d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (236607, 480)\n",
      "Total classes: 263\n",
      "Example classes: ['1.1.1' '1.1.2' '1.1.3' '1.1.5' '1.1.7' '1.1.9' '1.1.98' '1.1.99'\n",
      " '1.10.3' '1.10.5' '1.11.1' '1.11.2' '1.12.1' '1.12.2' '1.12.5' '1.12.7'\n",
      " '1.12.98' '1.12.99' '1.13.11' '1.13.12']\n",
      "Train size: 189285\n",
      "Test size : 47322\n",
      "Using device: cuda\n",
      "BiLSTMClassifier(\n",
      "  (lstm): LSTM(32, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=263, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 740/740 [00:17<00:00, 41.42it/s, loss=1.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=3.1566, acc=0.2978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 740/740 [00:17<00:00, 42.83it/s, loss=0.834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=1.1202, acc=0.7494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 740/740 [00:18<00:00, 41.06it/s, loss=0.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=0.6680, acc=0.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 740/740 [00:18<00:00, 39.46it/s, loss=0.483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss=0.4810, acc=0.8905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 740/740 [00:18<00:00, 40.26it/s, loss=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss=0.3764, acc=0.9130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 740/740 [00:19<00:00, 37.89it/s, loss=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss=0.3099, acc=0.9276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 740/740 [00:19<00:00, 37.82it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss=0.2630, acc=0.9371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 740/740 [00:19<00:00, 38.79it/s, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss=0.2294, acc=0.9450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 740/740 [00:18<00:00, 39.60it/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss=0.1998, acc=0.9509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 740/740 [00:18<00:00, 40.59it/s, loss=0.262] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss=0.1766, acc=0.9558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 740/740 [00:18<00:00, 41.00it/s, loss=0.131] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss=0.1586, acc=0.9601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 740/740 [00:18<00:00, 40.63it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss=0.1423, acc=0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 740/740 [00:18<00:00, 40.68it/s, loss=0.0962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss=0.1288, acc=0.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 740/740 [00:18<00:00, 40.41it/s, loss=0.0958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss=0.1158, acc=0.9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 740/740 [00:18<00:00, 40.01it/s, loss=0.0805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss=0.1051, acc=0.9721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 740/740 [00:18<00:00, 40.09it/s, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss=0.1011, acc=0.9729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 740/740 [00:18<00:00, 39.19it/s, loss=0.103] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss=0.0909, acc=0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 740/740 [00:18<00:00, 40.19it/s, loss=0.179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss=0.0825, acc=0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 740/740 [00:18<00:00, 40.32it/s, loss=0.0968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss=0.0768, acc=0.9788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 740/740 [00:18<00:00, 40.13it/s, loss=0.173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss=0.0692, acc=0.9807\n",
      "Model saved as 'bilstm_ec_esm2.pt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (BiLSTM): 100%|██████████| 185/185 [00:02<00:00, 70.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BiLSTM Test Results ===\n",
      "Test Accuracy (BiLSTM): 0.9581589958158996\n",
      "\n",
      "Classification report (BiLSTM, only classes present in test/pred):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       1.1.1       0.97      0.98      0.97      1401\n",
      "       1.1.2       0.50      1.00      0.67         1\n",
      "       1.1.3       0.72      0.95      0.82        19\n",
      "       1.1.5       0.92      0.95      0.94        64\n",
      "       1.1.9       0.00      0.00      0.00         2\n",
      "      1.1.98       1.00      1.00      1.00         6\n",
      "      1.1.99       0.94      0.82      0.88        40\n",
      "      1.10.3       0.99      0.96      0.97       119\n",
      "      1.10.5       1.00      1.00      1.00         1\n",
      "      1.11.1       0.84      0.94      0.89       224\n",
      "      1.11.2       1.00      0.50      0.67         6\n",
      "      1.12.1       0.25      0.33      0.29         3\n",
      "      1.12.2       0.00      0.00      0.00         1\n",
      "      1.12.7       1.00      0.33      0.50         3\n",
      "     1.12.98       0.50      0.29      0.36         7\n",
      "     1.12.99       1.00      0.78      0.88         9\n",
      "     1.13.11       0.87      0.89      0.88       152\n",
      "     1.13.12       1.00      0.40      0.57         5\n",
      "     1.13.99       1.00      1.00      1.00         3\n",
      "     1.14.11       0.88      0.93      0.90        97\n",
      "     1.14.12       0.88      0.97      0.92        30\n",
      "     1.14.13       0.90      0.85      0.88        89\n",
      "     1.14.14       0.89      0.94      0.91       183\n",
      "     1.14.15       0.85      0.79      0.81        28\n",
      "     1.14.16       0.50      0.20      0.29         5\n",
      "     1.14.17       1.00      0.44      0.62         9\n",
      "     1.14.18       0.75      0.75      0.75        12\n",
      "     1.14.19       0.90      0.71      0.80        66\n",
      "     1.14.20       0.50      0.25      0.33         4\n",
      "     1.14.99       0.87      0.94      0.91        71\n",
      "      1.15.1       0.81      0.88      0.84        92\n",
      "      1.16.1       1.00      0.86      0.92         7\n",
      "      1.16.3       0.97      0.94      0.95        32\n",
      "      1.17.1       0.93      0.98      0.96       117\n",
      "      1.17.2       0.00      0.00      0.00         2\n",
      "      1.17.4       0.83      0.60      0.70        40\n",
      "      1.17.5       0.00      0.00      0.00         1\n",
      "      1.17.7       0.99      0.98      0.99       169\n",
      "      1.17.8       1.00      1.00      1.00         1\n",
      "      1.17.9       0.00      0.00      0.00         1\n",
      "     1.17.98       1.00      0.50      0.67         2\n",
      "     1.17.99       0.96      0.92      0.94        50\n",
      "      1.18.1       1.00      0.89      0.94        55\n",
      "      1.18.6       0.94      0.82      0.87        38\n",
      "       1.2.1       0.98      0.99      0.98       645\n",
      "       1.2.3       1.00      0.33      0.50         6\n",
      "       1.2.4       1.00      0.93      0.97        61\n",
      "       1.2.5       0.00      0.00      0.00         0\n",
      "       1.2.7       0.94      0.89      0.91        35\n",
      "      1.2.99       0.00      0.00      0.00         2\n",
      "      1.20.4       1.00      1.00      1.00         8\n",
      "      1.21.1       1.00      0.50      0.67         2\n",
      "      1.21.3       0.75      0.75      0.75         4\n",
      "      1.21.4       0.75      0.50      0.60         6\n",
      "     1.21.98       1.00      1.00      1.00         8\n",
      "     1.21.99       1.00      1.00      1.00         7\n",
      "      1.23.1       0.50      1.00      0.67         1\n",
      "       1.3.1       0.96      0.87      0.91       263\n",
      "       1.3.2       1.00      1.00      1.00         3\n",
      "       1.3.3       0.98      0.93      0.96        46\n",
      "       1.3.5       0.99      0.99      0.99        85\n",
      "       1.3.7       0.94      0.92      0.93        78\n",
      "       1.3.8       0.90      1.00      0.95        19\n",
      "      1.3.98       0.64      1.00      0.78        23\n",
      "      1.3.99       0.56      0.62      0.59         8\n",
      "       1.4.1       0.95      0.89      0.92        61\n",
      "       1.4.3       0.96      0.95      0.95       121\n",
      "       1.4.4       1.00      0.99      1.00       117\n",
      "       1.4.7       0.67      0.67      0.67         3\n",
      "       1.4.9       1.00      1.00      1.00         1\n",
      "      1.4.99       0.00      0.00      0.00         1\n",
      "       1.5.1       0.97      0.95      0.96       242\n",
      "       1.5.3       0.83      0.56      0.67         9\n",
      "       1.5.5       1.00      1.00      1.00         5\n",
      "       1.5.7       0.00      0.00      0.00         1\n",
      "       1.5.8       1.00      1.00      1.00         1\n",
      "      1.5.98       1.00      0.55      0.71        11\n",
      "      1.5.99       0.40      0.50      0.44         4\n",
      "       1.6.1       1.00      0.94      0.97        16\n",
      "       1.6.2       1.00      1.00      1.00        20\n",
      "       1.6.3       0.67      0.80      0.73         5\n",
      "       1.6.5       1.00      0.88      0.94        66\n",
      "      1.6.99       0.89      0.89      0.89         9\n",
      "       1.7.1       0.95      0.87      0.91       188\n",
      "       1.7.2       1.00      0.97      0.99        35\n",
      "       1.7.3       1.00      0.67      0.80         3\n",
      "       1.7.5       0.75      1.00      0.86         3\n",
      "       1.7.7       1.00      0.33      0.50         3\n",
      "      1.7.99       0.81      1.00      0.90        30\n",
      "       1.8.1       0.88      0.96      0.92       113\n",
      "       1.8.2       0.00      0.00      0.00         0\n",
      "       1.8.3       0.80      0.80      0.80        15\n",
      "       1.8.4       0.97      0.88      0.92       141\n",
      "       1.8.5       1.00      0.33      0.50         9\n",
      "       1.8.7       0.62      0.50      0.56        10\n",
      "      1.8.98       1.00      0.62      0.77         8\n",
      "       1.9.6       0.97      1.00      0.98        30\n",
      "      1.97.1       0.98      0.97      0.97        99\n",
      "       2.1.1       0.95      0.98      0.97      2293\n",
      "       2.1.2       0.99      1.00      1.00       590\n",
      "       2.1.3       0.99      1.00      1.00       424\n",
      "       2.1.4       1.00      1.00      1.00         3\n",
      "      2.10.1       1.00      0.75      0.86        12\n",
      "       2.2.1       0.98      0.99      0.98       283\n",
      "       2.3.1       0.98      0.97      0.97      1778\n",
      "       2.3.2       0.90      0.94      0.92       531\n",
      "       2.3.3       0.86      0.96      0.91       171\n",
      "       2.4.1       0.97      0.95      0.96       733\n",
      "       2.4.2       0.99      0.98      0.98      1022\n",
      "       2.4.3       0.92      0.86      0.89        14\n",
      "      2.4.99       1.00      1.00      1.00       186\n",
      "       2.5.1       0.94      0.96      0.95      1572\n",
      "       2.6.1       0.94      0.98      0.96       358\n",
      "      2.6.99       1.00      1.00      1.00        51\n",
      "       2.7.1       0.98      0.95      0.97      1582\n",
      "      2.7.10       0.85      0.85      0.85       128\n",
      "      2.7.11       0.95      0.94      0.95       839\n",
      "      2.7.12       0.64      0.62      0.63        34\n",
      "      2.7.13       0.94      0.98      0.96       108\n",
      "      2.7.14       0.78      1.00      0.88        14\n",
      "       2.7.2       0.99      1.00      0.99       415\n",
      "       2.7.3       0.88      0.88      0.88        25\n",
      "       2.7.4       0.99      0.98      0.98       788\n",
      "       2.7.6       0.90      0.99      0.94        70\n",
      "       2.7.7       0.97      0.97      0.97      2462\n",
      "       2.7.8       0.98      0.99      0.99       391\n",
      "       2.7.9       1.00      1.00      1.00        28\n",
      "       2.8.1       0.99      0.99      0.99       652\n",
      "       2.8.2       1.00      1.00      1.00        28\n",
      "       2.8.3       0.95      0.95      0.95        22\n",
      "       2.8.4       1.00      0.98      0.99       217\n",
      "       2.8.5       1.00      0.67      0.80         3\n",
      "       2.9.1       0.98      0.96      0.97        57\n",
      "       3.1.1       0.92      0.96      0.94       758\n",
      "      3.1.11       1.00      0.99      1.00       192\n",
      "      3.1.12       1.00      0.67      0.80         6\n",
      "      3.1.13       1.00      1.00      1.00        41\n",
      "      3.1.16       1.00      1.00      1.00         1\n",
      "       3.1.2       0.99      0.97      0.98        95\n",
      "      3.1.21       0.99      0.92      0.95       249\n",
      "      3.1.22       0.00      0.00      0.00         1\n",
      "      3.1.26       0.98      0.98      0.98       524\n",
      "       3.1.3       0.98      0.92      0.95       676\n",
      "      3.1.30       0.50      1.00      0.67         1\n",
      "      3.1.31       1.00      1.00      1.00         2\n",
      "       3.1.4       0.76      0.83      0.79        90\n",
      "       3.1.5       1.00      1.00      1.00         6\n",
      "       3.1.6       1.00      0.69      0.82        13\n",
      "       3.1.7       1.00      0.12      0.22         8\n",
      "       3.1.8       0.00      0.00      0.00         0\n",
      "      3.11.1       0.94      1.00      0.97        15\n",
      "      3.13.1       0.00      0.00      0.00         3\n",
      "      3.13.2       0.93      0.98      0.95        43\n",
      "       3.2.1       0.91      0.94      0.92       669\n",
      "       3.2.2       0.95      0.91      0.93       286\n",
      "       3.3.2       0.79      0.65      0.71        23\n",
      "      3.4.11       0.97      0.99      0.98       228\n",
      "      3.4.13       0.98      0.92      0.95        49\n",
      "      3.4.14       1.00      0.88      0.94        33\n",
      "      3.4.15       1.00      0.80      0.89         5\n",
      "      3.4.16       1.00      0.95      0.98        43\n",
      "      3.4.17       0.91      0.95      0.93        21\n",
      "      3.4.19       0.97      0.92      0.95       157\n",
      "      3.4.21       1.00      0.96      0.98       444\n",
      "      3.4.22       0.85      0.90      0.87        67\n",
      "      3.4.23       1.00      0.98      0.99       131\n",
      "      3.4.24       0.94      0.92      0.93        72\n",
      "      3.4.25       0.96      0.91      0.94       139\n",
      "       3.5.1       0.95      0.96      0.96       887\n",
      "       3.5.2       0.98      0.95      0.96       239\n",
      "       3.5.3       0.91      0.95      0.93       137\n",
      "       3.5.4       0.88      0.92      0.90       516\n",
      "       3.5.5       1.00      0.31      0.47        13\n",
      "      3.5.99       0.92      0.97      0.95        72\n",
      "       3.6.1       0.97      0.97      0.97       799\n",
      "       3.6.4       0.95      0.95      0.95       384\n",
      "       3.6.5       1.00      0.99      1.00       344\n",
      "       3.7.1       0.94      0.92      0.93        37\n",
      "       3.8.1       0.62      0.50      0.55        16\n",
      "       3.9.1       0.00      0.00      0.00         0\n",
      "       4.1.1       0.91      0.97      0.94      1040\n",
      "       4.1.2       0.96      0.94      0.95       198\n",
      "       4.1.3       0.93      0.87      0.90       148\n",
      "      4.1.99       0.92      0.94      0.93       217\n",
      "       4.2.1       0.95      0.95      0.95      1612\n",
      "       4.2.2       0.88      0.90      0.89        48\n",
      "       4.2.3       0.97      0.95      0.96       415\n",
      "      4.2.99       0.92      0.87      0.90        54\n",
      "       4.3.1       0.80      0.92      0.86       118\n",
      "       4.3.2       0.92      0.96      0.94       312\n",
      "       4.3.3       1.00      0.99      1.00       136\n",
      "      4.3.99       0.88      1.00      0.93         7\n",
      "       4.4.1       0.83      0.85      0.84       118\n",
      "       4.6.1       0.89      0.90      0.89       275\n",
      "       4.8.1       1.00      0.50      0.67         2\n",
      "      4.98.1       0.99      0.99      0.99        75\n",
      "      4.99.1       0.91      0.86      0.88        35\n",
      "       5.1.1       0.98      0.97      0.97       248\n",
      "       5.1.2       0.00      0.00      0.00         1\n",
      "       5.1.3       0.91      0.95      0.93       155\n",
      "      5.1.99       0.95      0.91      0.93        22\n",
      "       5.2.1       0.92      0.94      0.93       307\n",
      "       5.3.1       0.98      0.98      0.98       825\n",
      "       5.3.2       0.81      0.81      0.81        16\n",
      "       5.3.3       0.95      0.91      0.93        69\n",
      "       5.3.4       0.96      0.92      0.94        24\n",
      "      5.3.99       0.50      0.57      0.53         7\n",
      "       5.4.2       1.00      1.00      1.00       390\n",
      "       5.4.3       0.99      1.00      1.00       147\n",
      "       5.4.4       1.00      0.67      0.80         3\n",
      "      5.4.99       0.96      0.94      0.95       386\n",
      "       5.5.1       0.72      0.60      0.66        43\n",
      "       5.6.1       0.99      0.99      0.99       219\n",
      "       5.6.2       0.95      0.92      0.93       212\n",
      "       6.1.1       1.00      1.00      1.00      2576\n",
      "       6.2.1       0.98      0.98      0.98       277\n",
      "       6.3.1       0.99      0.97      0.98       231\n",
      "       6.3.2       0.97      0.99      0.98       716\n",
      "       6.3.3       0.99      0.99      0.99       138\n",
      "       6.3.4       0.98      0.97      0.97       788\n",
      "       6.3.5       0.99      0.98      0.99       400\n",
      "       6.4.1       1.00      0.80      0.89        10\n",
      "       6.5.1       0.98      0.96      0.97       253\n",
      "       6.6.1       1.00      0.92      0.96        13\n",
      "       6.7.1       0.00      0.00      0.00         2\n",
      "       7.1.1       0.99      0.96      0.97       464\n",
      "       7.1.2       0.99      0.98      0.99       397\n",
      "       7.1.3       0.33      0.75      0.46         4\n",
      "       7.2.1       0.97      0.96      0.97       102\n",
      "       7.2.2       0.97      0.90      0.93       108\n",
      "       7.2.3       0.00      0.00      0.00         6\n",
      "       7.2.4       1.00      1.00      1.00        15\n",
      "       7.3.2       0.94      0.95      0.95       127\n",
      "       7.4.2       0.96      0.98      0.97       217\n",
      "       7.5.2       0.99      0.95      0.97        79\n",
      "       7.6.2       0.93      0.94      0.94       139\n",
      "\n",
      "    accuracy                           0.96     47322\n",
      "   macro avg       0.85      0.81      0.82     47322\n",
      "weighted avg       0.96      0.96      0.96     47322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# =====================================================\n",
    "# 1. Load ESM2 embeddings & labels\n",
    "# =====================================================\n",
    "X = np.load(\"esm2_features.npy\")  # shape = (236607, 480)\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "df = pd.read_csv(\"3_levels_EC.tsv\", sep=\"\\t\")\n",
    "labels = df[\"EC number\"].astype(str).values\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Total classes:\", num_classes)\n",
    "print(\"Example classes:\", le.classes_[:20])\n",
    "\n",
    "# =====================================================\n",
    "# 2. Train-test split\n",
    "# =====================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=None  # jangan stratify karena ada kelas sangat langka\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size :\", X_test.shape[0])\n",
    "\n",
    "# =====================================================\n",
    "# 3. Dataset & DataLoader (reshape ke pseudo-sequence)\n",
    "# =====================================================\n",
    "SEQ_LEN = 15\n",
    "EMB_DIM = 32  # 15 × 32 = 480\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # reshape (N, 480) -> (N, 15, 32)\n",
    "        X = X.reshape(len(X), SEQ_LEN, EMB_DIM)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = ProteinDataset(X_train, y_train)\n",
    "test_ds  = ProteinDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "# =====================================================\n",
    "# 4. BiLSTM model\n",
    "# =====================================================\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim=32, hidden_size=128, num_layers=2, num_classes=263):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # karena bidirectional -> hidden_size * 2\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len=15, embed_dim=32)\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        # hn shape: (num_layers*2, batch, hidden_size)\n",
    "        # pakai last layer, dua arah: hn[-2] (forward), hn[-1] (backward)\n",
    "        final = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1)  # (batch, hidden_size*2)\n",
    "        logits = self.fc(final)\n",
    "        return logits\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = BiLSTMClassifier(\n",
    "    embed_dim=EMB_DIM,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# =====================================================\n",
    "# 5. Optimizer & Loss\n",
    "# =====================================================\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "# =====================================================\n",
    "# 6. Training loop with tqdm\n",
    "# =====================================================\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for Xb, yb in pbar:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch {epoch}: loss={train_loss:.4f}, acc={train_acc:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# 7. Save model\n",
    "# =====================================================\n",
    "torch.save(model.state_dict(), \"bilstm_ec_esm2.pt\")\n",
    "print(\"Model saved as 'bilstm_ec_esm2.pt'\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. Evaluation: test accuracy + classification_report\n",
    "# =====================================================\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in tqdm(test_loader, desc=\"Evaluating (BiLSTM)\"):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        logits = model(Xb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(yb.cpu().numpy())\n",
    "\n",
    "y_true = np.concatenate(all_targets)\n",
    "y_pred = np.concatenate(all_preds)\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "print(\"\\n=== BiLSTM Test Results ===\")\n",
    "print(\"Test Accuracy (BiLSTM):\", test_acc)\n",
    "\n",
    "# Hanya kelas yang muncul di y_true/y_pred supaya tidak error\n",
    "labels_used = np.unique(np.concatenate([y_true, y_pred]))\n",
    "target_names = le.inverse_transform(labels_used)\n",
    "\n",
    "print(\"\\nClassification report (BiLSTM, only classes present in test/pred):\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=labels_used,\n",
    "    target_names=target_names,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce03dfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIM: 480\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BiLSTMClassifier:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1052, 480]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1052, 263]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1052, 480]).\n\tsize mismatch for lstm.weight_hh_l0_reverse: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1052, 263]).\n\tsize mismatch for lstm.bias_ih_l0_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.bias_hh_l0_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1052, 526]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1052, 263]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.weight_ih_l1_reverse: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1052, 526]).\n\tsize mismatch for lstm.weight_hh_l1_reverse: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1052, 263]).\n\tsize mismatch for lstm.bias_ih_l1_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.bias_hh_l1_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([263, 256]) from checkpoint, the shape in current model is torch.Size([263, 526]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m BiLSTMClassifier(INPUT_DIM, num_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilstm_ec_esm2.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP model loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2585\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2589\u001b[0m             ),\n\u001b[0;32m   2590\u001b[0m         )\n\u001b[0;32m   2592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2595\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2596\u001b[0m         )\n\u001b[0;32m   2597\u001b[0m     )\n\u001b[0;32m   2598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BiLSTMClassifier:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1052, 480]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1052, 263]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1052, 480]).\n\tsize mismatch for lstm.weight_hh_l0_reverse: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1052, 263]).\n\tsize mismatch for lstm.bias_ih_l0_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.bias_hh_l0_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1052, 526]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1052, 263]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.weight_ih_l1_reverse: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([1052, 526]).\n\tsize mismatch for lstm.weight_hh_l1_reverse: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1052, 263]).\n\tsize mismatch for lstm.bias_ih_l1_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for lstm.bias_hh_l1_reverse: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1052]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([263, 256]) from checkpoint, the shape in current model is torch.Size([263, 526])."
     ]
    }
   ],
   "source": [
    "features = np.load(\"esm2_features.npy\")  \n",
    "INPUT_DIM = features.shape[1]\n",
    "print(\"INPUT_DIM:\", INPUT_DIM)\n",
    "\n",
    "model = BiLSTMClassifier(INPUT_DIM, num_classes).to(device)\n",
    "\n",
    "state_dict = torch.load(\"bilstm_ec_esm2.pt\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "print(\"MLP model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eac5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ESM2 model: facebook/esm2_t12_35M_UR50D\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"facebook/esm2_t12_35M_UR50D\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "esm_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "esm_model.to(device)\n",
    "esm_model.eval()\n",
    "\n",
    "print(\"Loaded ESM2 model:\", MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c131224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def embed_seq(seq: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed 1 protein sequence using ESM2 t12 mean pooling.\n",
    "    seq: string asam amino (A,C,D,...), tanpa spasi.\n",
    "    return: numpy array shape (INPUT_DIM,)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(\n",
    "        [seq],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "    outputs = esm_model(**tokens)           # last_hidden_state: [1, L, D]\n",
    "    last_hidden = outputs.last_hidden_state # [1, L, D]\n",
    "    mask = tokens[\"attention_mask\"].unsqueeze(-1)  # [1, L, 1]\n",
    "\n",
    "    masked = last_hidden * mask             # [1, L, D]\n",
    "    summed = masked.sum(dim=1)              # [1, D]\n",
    "    counts = mask.sum(dim=1)                # [1, 1]\n",
    "    emb = (summed / counts).cpu().numpy()[0]    # [D]\n",
    "\n",
    "    # cek dimensi\n",
    "    if emb.shape[0] != INPUT_DIM:\n",
    "        raise ValueError(\n",
    "            f\"Dimension embeddings ({emb.shape[0]}) != INPUT_DIM MLP ({INPUT_DIM}). \"\n",
    "            f\"Please retrain the MLP model with the correct INPUT_DIM.\"\n",
    "        )\n",
    "\n",
    "    return emb\n",
    "@torch.no_grad()\n",
    "def predict_ec(seq: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Predict EC number from one protein sequence.\n",
    "    seq  : string amino acid (A,C,D,...)\n",
    "    top_k: print top-k predictions\n",
    "    return: (pred_ec, conf)\n",
    "    \"\"\"\n",
    "    emb = embed_seq(seq)                                      # (D,)\n",
    "    x = torch.tensor(emb, dtype=torch.float32, device=device).unsqueeze(0)  # (1, D)\n",
    "\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)[0].cpu().numpy()     # (num_classes,)\n",
    "\n",
    "    top_idx = probs.argsort()[::-1][:top_k]\n",
    "    top_ec  = label_encoder.inverse_transform(top_idx)\n",
    "    top_conf = probs[top_idx]\n",
    "\n",
    "    print(\"Top predictions:\")\n",
    "    for ec, p in zip(top_ec, top_conf):\n",
    "        print(f\"  {ec:10s}  (confidence = {p:.4f})\")\n",
    "\n",
    "    return top_ec[0], top_conf[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b33f5f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m TEST_SEQ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput sequence protein (A,C,D,... without space): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m----> 2\u001b[0m pred_ec, conf \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_ec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_SEQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted EC :\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_ec)\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 45\u001b[0m, in \u001b[0;36mpredict_ec\u001b[1;34m(seq, top_k)\u001b[0m\n\u001b[0;32m     42\u001b[0m emb \u001b[38;5;241m=\u001b[39m embed_seq(seq)                                      \u001b[38;5;66;03m# (D,)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(emb, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (1, D)\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()     \u001b[38;5;66;03m# (num_classes,)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m top_idx \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39margsort()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:top_k]\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 91\u001b[0m, in \u001b[0;36mBiLSTMClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     88\u001b[0m out, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# hn shape: (num_layers*2, batch, hidden_size)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# pakai last layer, dua arah: hn[-2] (forward), hn[-1] (backward)\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m final \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[43mhn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, hn[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch, hidden_size*2)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(final)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "TEST_SEQ = input(\"Input sequence protein (A,C,D,... without space): \").strip()\n",
    "pred_ec, conf = predict_ec(TEST_SEQ, top_k=3)\n",
    "\n",
    "print(\"\\nFinal prediction:\")\n",
    "print(\"Predicted EC :\", pred_ec)\n",
    "print(\"Confidence   :\", conf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
